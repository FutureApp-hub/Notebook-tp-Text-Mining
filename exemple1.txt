•	Sélection du corpus : cette étape consiste en la préparation du texte avant son exploitation. Récupérer les données textuelles de différentes sources et les rassembler au même endroit. Si le texte apparaît dans plusieurs fichiers, les enregistrer au même endroit. Dans le cas es bases de données, déterminer le (les) champ (s) contenant le texte à analyser.
•	Analyse et extraction des données structurées : Cette étape consiste à appliquer des algorithmes de prétraitement (tokenisation, suppressions des mots vide de sens (stops words), des espaces inutiles, des ponctuations, suppression des signes du langage, lemmatisation, racinisation, etc) au texte précédent dans le but de le nettoyer.
•	Réduction de dimension (Bag-of-words, Word2Vec, …) : Elle consiste à passer de la représentation non structurée de nos documents à un format structurer sur lequel nous pouvant appliquer nos algorithmes usuels de machine learning et de deep learning.
•	Analyse des données structurées : Une fois l’étape précédente terminer, nous pouvant appliquer un algorithme de machine learning (classification, clustering etc …) ou de deep learning (lstm, bert, …) de notre choix en fonction du domaine d’application dans lequel on se trouve.
•	Visualisation des résultats : Pour un algorithme de classification supervisée, cette étape consiste à regarder le comportement de notre modèle sur de nouvelles données ; Et pour de la classification non supervisée à analyser les différents clusters (taille, individu, proportion, …)
•	Interprétation : Etape de prise de décision.
